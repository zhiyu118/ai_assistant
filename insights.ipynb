{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform CSV into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers = []\n",
    "file_names = [\"active_tracker.csv\", \"purged_tracker.csv\"]\n",
    "\n",
    "for file in file_names:\n",
    "    header_index = None\n",
    "    with open(file, \"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for i, row in enumerate(reader):\n",
    "            if any((cell or \"\").strip() == \"In CRM?\" for cell in row):\n",
    "                header_index = i\n",
    "                break\n",
    "    if header_index is None:\n",
    "        raise ValueError(f\"Could not find a row containing 'In CRM?' in any cell of {file}.\")\n",
    "    df = pd.read_csv(file, skiprows=header_index, header=0)\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    df = df.loc[:, ~df.columns.astype(str).str.contains(\"`\")]\n",
    "    trackers.append(df)\n",
    "\n",
    "tracker = pd.concat(trackers, ignore_index=True)\n",
    "tracker = tracker.drop(columns=[\"General activities\", \"Project.1\"], errors=\"ignore\")\n",
    "tracker = tracker[~tracker[\"Status & health\"].isin([\"DUPLICATE\", \"LOST\"])]\n",
    "tracker = tracker.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "# --- Fuzzy match the 'derived/implied' revenue column (KEEP REST UNCHANGED) ---\n",
    "def _norm(col) -> str:\n",
    "    s = str(col).lower()\n",
    "    s = (s.replace(\"\\u00a0\", \" \")\n",
    "           .replace(\"\\r\", \" \")\n",
    "           .replace(\"\\n\", \" \"))\n",
    "    return \" \".join(s.split())  # collapse whitespace\n",
    "\n",
    "_tokens_all = [\"total\", \"revenue\", \"usd\"]\n",
    "_tokens_any = [\"derived\", \"implied\"]\n",
    "\n",
    "_candidates = [\n",
    "    c for c in tracker.columns\n",
    "    if all(tok in _norm(c) for tok in _tokens_all) and any(tok in _norm(c) for tok in _tokens_any)\n",
    "]\n",
    "if not _candidates:\n",
    "    raise KeyError(\"Could not find the 'derived/implied' revenue column by fuzzy match.\")\n",
    "derived = _candidates[0]  # use the first (only) match\n",
    "\n",
    "def to_number(s: pd.Series) -> pd.Series:\n",
    "    t = s.astype(str)\n",
    "    t = t.str.replace(\"\\u00a0\", \" \", regex=False)     # NBSP -> space\n",
    "    t = t.str.replace(\",\", \"\", regex=False)           # remove commas\n",
    "    t = t.str.replace(\"$\", \"\", regex=False)           # remove $\n",
    "    t = t.str.replace(r\"^\\((.*)\\)$\", r\"-\\1\", regex=True)  # (123) -> -123\n",
    "    # extract first numeric token and convert; no digits -> NaN\n",
    "    return pd.to_numeric(t.str.extract(r\"(-?\\d+(?:\\.\\d+)?)\", expand=False), errors=\"coerce\")\n",
    "\n",
    "# Only use the derived/implied column; no fallback to declared\n",
    "derived_num = to_number(tracker[derived])\n",
    "\n",
    "tracker[\"Total revenue\"] = (\n",
    "    pd.Series(derived_num.where(derived_num > 0, 0), index=tracker.index)\n",
    "      .fillna(0)\n",
    "      .astype(int)\n",
    ")\n",
    "\n",
    "# --- Diagnostics (entire tracker) ---\n",
    "# Count rows where the source (derived) is strictly positive\n",
    "count_pos_source = int((derived_num.notna() & (derived_num > 0)).sum())\n",
    "# Count rows where Total revenue is strictly positive\n",
    "count_pos_total  = int((tracker[\"Total revenue\"] > 0).sum())\n",
    "\n",
    "print(\"Rows where derived has a positive number (tracker):\", count_pos_source)\n",
    "print(\"Rows with positive Total revenue (tracker):\",        count_pos_total)\n",
    "\n",
    "# A) derived > 0 but Total revenue == 0  (should be 0 under current rule)\n",
    "mask_a = (derived_num > 0) & tracker[\"Total revenue\"].eq(0)\n",
    "if mask_a.any():\n",
    "    print(\"\\nExamples where derived>0 but Total revenue == 0:\")\n",
    "    print(tracker.loc[mask_a, [derived, \"Total revenue\"]].head(10))\n",
    "\n",
    "# B) derived <= 0 or NaN but Total revenue > 0  (should be 0 under current rule)\n",
    "mask_b = ((derived_num.isna()) | (derived_num <= 0)) & tracker[\"Total revenue\"].gt(0)\n",
    "if mask_b.any():\n",
    "    print(\"\\nExamples where derived<=0/NaN but Total revenue > 0:\")\n",
    "    print(tracker.loc[mask_b, [derived, \"Total revenue\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ongoing = tracker[tracker[\"Status & health\"] == \"ONGOING\"].copy()\n",
    "complete = tracker[tracker[\"Status & health\"] == \"COMPLETE\"].copy()\n",
    "suspended = tracker[tracker[\"Status & health\"] == \"SUSPENDED\"].copy()\n",
    "\n",
    "print(\"Ongoing shape:\", ongoing.shape)\n",
    "print(\"Completed shape:\", complete.shape)\n",
    "print(\"Suspended shape:\", suspended.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker['Product\\nline'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revenue Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete - major product lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Config -----------------\n",
    "product_col = 'Product\\nline'\n",
    "date_col    = 'Updated'\n",
    "value_col   = 'Total revenue'\n",
    "\n",
    "# Source dataframe expected to exist\n",
    "sdf = complete.copy()\n",
    "\n",
    "# ---------- 1) Clean + parse dates robustly ----------\n",
    "sdf[date_col] = (\n",
    "    sdf[date_col]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "    .str.replace(\"\\r\", \" \", regex=False)\n",
    "    .str.replace(\"\\n\", \" \", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "parsed = pd.to_datetime(sdf[date_col], errors=\"coerce\")\n",
    "\n",
    "# Fallback: expand 2-digit years m/d/yy -> m/d/20yy\n",
    "mask_na = parsed.isna()\n",
    "if mask_na.any():\n",
    "    fixed_2yy = sdf.loc[mask_na, date_col].str.replace(\n",
    "        r\"^(\\d{1,2})/(\\d{1,2})/(\\d{2})$\",\n",
    "        r\"\\1/\\2/20\\3\",\n",
    "        regex=True\n",
    "    )\n",
    "    parsed.loc[mask_na] = pd.to_datetime(fixed_2yy, errors=\"coerce\")\n",
    "\n",
    "sdf[date_col] = parsed\n",
    "sdf = sdf.dropna(subset=[date_col])  # keep only rows with valid dates\n",
    "\n",
    "# Ensure revenue numeric\n",
    "sdf[value_col] = pd.to_numeric(sdf[value_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Drop missing product line rows\n",
    "sdf = sdf.dropna(subset=[product_col])\n",
    "\n",
    "# ---------- 2) Map Product line -> Product group by prefix ----------\n",
    "pl_norm = sdf[product_col].astype(str).str.strip().str.lower()\n",
    "\n",
    "group = np.select(\n",
    "    [\n",
    "        pl_norm.str.startswith('event'),\n",
    "        pl_norm.str.startswith('viewables'),\n",
    "        pl_norm.str.startswith('home'),\n",
    "        pl_norm.str.startswith('others'),\n",
    "        pl_norm.str.startswith('wearable'),\n",
    "        pl_norm.str.startswith('hearable'),\n",
    "    ],\n",
    "    ['Event', 'Viewables', 'Home', 'Others', 'Wearable', 'Hearable'],\n",
    "    default='Other'   # fallback for anything else\n",
    ")\n",
    "sdf['Product group'] = group\n",
    "\n",
    "# ---------- 3) Quarter labels & full range ----------\n",
    "sdf['quarter'] = sdf[date_col].dt.to_period('Q')\n",
    "\n",
    "if not sdf.empty:\n",
    "    q_start = sdf['quarter'].min()\n",
    "    q_end   = sdf['quarter'].max()\n",
    "    all_quarters = pd.period_range(q_start, q_end, freq='Q')\n",
    "else:\n",
    "    all_quarters = pd.PeriodIndex([], freq='Q')\n",
    "\n",
    "# ---------- 4) Aggregate revenue by quarter x product group ----------\n",
    "rev_q_pg = (\n",
    "    sdf.groupby(['quarter', 'Product group'], dropna=False)[value_col]\n",
    "       .sum()\n",
    "       .reset_index(name='revenue')\n",
    ")\n",
    "\n",
    "# Per-quarter totals and percentages\n",
    "totals = (rev_q_pg.groupby('quarter', as_index=False)['revenue']\n",
    "                   .sum()\n",
    "                   .rename(columns={'revenue': 'q_total'}))\n",
    "rev_q_pg = rev_q_pg.merge(totals, on='quarter', how='left')\n",
    "rev_q_pg['pct'] = np.where(\n",
    "    rev_q_pg['q_total'] > 0,\n",
    "    100 * rev_q_pg['revenue'] / rev_q_pg['q_total'],\n",
    "    0.0\n",
    ")\n",
    "\n",
    "# ---------- 5) Pivot to % share per group (all groups shown) ----------\n",
    "pct_pivot = rev_q_pg.pivot_table(\n",
    "    index='quarter',\n",
    "    columns='Product group',\n",
    "    values='pct',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ").reindex(all_quarters, fill_value=0)\n",
    "\n",
    "# Optional: ensure rows sum to ~100 (floating rounding may lead to 99.999/100.001)\n",
    "# Not strictly necessary; mapping includes an 'Other' fallback already.\n",
    "\n",
    "# ---------- 6) Plot stacked % bars (left y-axis) + total revenue line (right y-axis) ----------\n",
    "quarters_sorted = list(pct_pivot.index.astype(str))\n",
    "x = np.arange(len(quarters_sorted)) * 1.25\n",
    "bar_width = 0.8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# Stable color mapping by group name; force \"Other\" (fallback) to gray\n",
    "groups = list(pct_pivot.columns)\n",
    "n_series = len(groups)\n",
    "cmap = (plt.cm.get_cmap('tab20', n_series) if n_series <= 20 else plt.cm.get_cmap('hsv', n_series))\n",
    "group_colors = {\n",
    "    g: ((0.6, 0.6, 0.6, 1.0) if g == 'Other' else cmap(i))\n",
    "    for i, g in enumerate(groups)\n",
    "}\n",
    "\n",
    "# Order stacks by global % contribution for a stable legend/stack order\n",
    "col_order = pct_pivot.sum(axis=0).sort_values(ascending=False).index.tolist()\n",
    "\n",
    "bottom = np.zeros(len(pct_pivot), dtype=float)\n",
    "for g in col_order:\n",
    "    vals = pct_pivot[g].values\n",
    "    if np.any(vals != 0):\n",
    "        ax.bar(x, vals, width=bar_width, bottom=bottom, label=g, color=group_colors[g])\n",
    "        bottom += vals\n",
    "\n",
    "# Left y-axis: percentage\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel(\"Percent of Total Revenue\")\n",
    "\n",
    "# X-axis formatting\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(quarters_sorted, rotation=45, ha=\"right\")\n",
    "ax.tick_params(axis=\"x\", labelsize=9, pad=8)\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "ax.margins(x=0.02)\n",
    "\n",
    "ax.set_title(\"Share of Revenue and Total Revenue per Quarter\")\n",
    "\n",
    "# Right y-axis: quarter totals as a line\n",
    "qtot = totals.set_index('quarter')['q_total'].reindex(all_quarters).fillna(0)\n",
    "ax2 = ax.twinx()\n",
    "line_plot, = ax2.plot(x, qtot.values, marker='o', linewidth=2, color='#0b1e3f', label=\"Quarter Total\")\n",
    "ax2.set_ylabel(\"Total Revenue (Billion USD)\")\n",
    "\n",
    "# Give extra right margin for the legend\n",
    "fig.subplots_adjust(right=0.78)\n",
    "\n",
    "# Combined legend (bars + line), placed further right\n",
    "handles1, labels1 = ax.get_legend_handles_labels()\n",
    "ax.legend(\n",
    "    handles1 + [line_plot],\n",
    "    labels1 + [\"Quarter Total\"],\n",
    "    title=\"Product Lines\",\n",
    "    bbox_to_anchor=(1.05, 1.00),   # farther from plot\n",
    "    loc=\"upper left\",\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete - specific product lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_col = 'Product\\nline'\n",
    "date_col    = 'Updated'\n",
    "value_col   = 'Total revenue'\n",
    "\n",
    "sdf = complete.copy()\n",
    "\n",
    "# --- 1) Clean + parse dates robustly (handles 1/2-digit M/D and 2- or 4-digit years) ---\n",
    "sdf[date_col] = (\n",
    "    sdf[date_col]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "    .str.replace(\"\\r\", \" \", regex=False)\n",
    "    .str.replace(\"\\n\", \" \", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# first pass (flexible)\n",
    "parsed = pd.to_datetime(sdf[date_col], errors=\"coerce\")\n",
    "\n",
    "# fallback: expand 2-digit years like m/d/yy -> m/d/20yy (e.g., 9/5/25 -> 9/5/2025)\n",
    "mask_na = parsed.isna()\n",
    "if mask_na.any():\n",
    "    fixed_2yy = sdf.loc[mask_na, date_col].str.replace(\n",
    "        r\"^(\\d{1,2})/(\\d{1,2})/(\\d{2})$\",\n",
    "        r\"\\1/\\2/20\\3\",\n",
    "        regex=True\n",
    "    )\n",
    "    parsed.loc[mask_na] = pd.to_datetime(fixed_2yy, errors=\"coerce\")\n",
    "\n",
    "sdf[date_col] = parsed\n",
    "# keep only rows with a valid date\n",
    "sdf = sdf.dropna(subset=[date_col])\n",
    "\n",
    "# ensure revenue numeric\n",
    "sdf[value_col] = pd.to_numeric(sdf[value_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# optional: drop missing product line rows\n",
    "sdf = sdf.dropna(subset=[product_col])\n",
    "\n",
    "# --- 2) Quarter labels and completeness over time range ---\n",
    "sdf['quarter'] = sdf[date_col].dt.to_period('Q')\n",
    "\n",
    "# sum by quarter x product line\n",
    "rev_q_pl = (\n",
    "    sdf.groupby(['quarter', product_col], dropna=False)[value_col]\n",
    "       .sum()\n",
    "       .reset_index(name='revenue')\n",
    ")\n",
    "\n",
    "# per-quarter totals and percentages\n",
    "totals = rev_q_pl.groupby('quarter', as_index=False)['revenue'].sum().rename(columns={'revenue':'q_total'})\n",
    "rev_q_pl = rev_q_pl.merge(totals, on='quarter', how='left')\n",
    "rev_q_pl['pct'] = np.where(rev_q_pl['q_total'] > 0, 100 * rev_q_pl['revenue'] / rev_q_pl['q_total'], 0.0)\n",
    "\n",
    "# --- Build a complete PeriodIndex of quarters (unique) ---\n",
    "if not sdf.empty:\n",
    "    q_start = sdf['quarter'].min()\n",
    "    q_end   = sdf['quarter'].max()\n",
    "    all_quarters = pd.period_range(q_start, q_end, freq='Q')\n",
    "else:\n",
    "    all_quarters = pd.PeriodIndex([], freq='Q')\n",
    "\n",
    "# --- Keep top 3 product lines per quarter by percentage ---\n",
    "top3 = (\n",
    "    rev_q_pl.sort_values(['quarter', 'pct'], ascending=[True, False])\n",
    "            .groupby('quarter', as_index=False)\n",
    "            .head(3)\n",
    ")\n",
    "\n",
    "# === Build PERCENT stacks: top-3 % + \"Other\" so each bar reaches 100% ===\n",
    "pct_pivot = top3.pivot_table(\n",
    "    index='quarter',\n",
    "    columns=product_col,\n",
    "    values='pct',        # percentages\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Ensure all quarters present\n",
    "pct_pivot = pct_pivot.reindex(all_quarters, fill_value=0)\n",
    "\n",
    "# Add \"Other\" percentage per quarter to reach 100\n",
    "row_sum = pct_pivot.sum(axis=1).clip(0, 100)\n",
    "pct_pivot['Other'] = (100 - row_sum).clip(lower=0, upper=100)\n",
    "\n",
    "# Order legend/stack by global contribution (sum of % across quarters)\n",
    "col_order = pct_pivot.sum(axis=0).sort_values(ascending=False).index.tolist()\n",
    "# Put \"Other\" at the end for readability\n",
    "if 'Other' in col_order:\n",
    "    col_order = [c for c in col_order if c != 'Other'] + ['Other']\n",
    "\n",
    "# --- Plot stacked bars in PERCENTAGES (left y-axis) ---\n",
    "quarters_sorted = list(pct_pivot.index.astype(str))\n",
    "\n",
    "spacing = 1.25\n",
    "x = np.arange(len(quarters_sorted)) * spacing\n",
    "bar_width = 0.8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# Colors; force \"Other\" to gray\n",
    "n_series = len(col_order)\n",
    "cmap = (plt.cm.get_cmap('tab20', n_series) if n_series <= 20\n",
    "        else plt.cm.get_cmap('hsv', n_series))\n",
    "series_colors = {name: (0.6, 0.6, 0.6, 1.0) if name == 'Other' else cmap(i)\n",
    "                 for i, name in enumerate(col_order)}\n",
    "\n",
    "bottom = np.zeros(len(pct_pivot), dtype=float)\n",
    "for name in col_order:\n",
    "    vals = pct_pivot[name].values\n",
    "    if np.any(vals != 0):\n",
    "        ax.bar(x, vals, width=bar_width, bottom=bottom, label=name, color=series_colors[name])\n",
    "        bottom += vals\n",
    "\n",
    "# Left y-axis: percentage\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel(\"Percent of Total Revenue\")\n",
    "\n",
    "# X-axis labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(quarters_sorted, rotation=45, ha=\"right\")\n",
    "ax.tick_params(axis=\"x\", labelsize=9, pad=8)\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "ax.margins(x=0.02)\n",
    "\n",
    "ax.set_title(\"Share of Revenue and Total Revenue per Quarter\")\n",
    "\n",
    "# --- Overlay line of quarter totals on RIGHT y-axis ---\n",
    "qtot = totals.set_index('quarter')['q_total'].reindex(all_quarters).fillna(0)\n",
    "ax2 = ax.twinx()\n",
    "# Dark navy line\n",
    "line_plot, = ax2.plot(x, qtot.values, marker='o', linewidth=2, color='#0b1e3f', label=\"Quarter Total\")\n",
    "ax2.set_ylabel(\"Total Revenue (Billion USD)\")\n",
    "\n",
    "# Give extra right margin for the farther legend\n",
    "fig.subplots_adjust(right=0.78)\n",
    "\n",
    "# --- Combined legend (bars + line), moved farther right ---\n",
    "handles1, labels1 = ax.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(\n",
    "    handles1 + [line_plot],\n",
    "    labels1 + [\"Quarter Total\"],\n",
    "    title=\"Product Lines\",\n",
    "    bbox_to_anchor=(1.05, 1.00),\n",
    "    loc=\"upper left\",\n",
    "    frameon=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete - specific BUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_col = 'Owner unit'\n",
    "date_col    = 'Updated'\n",
    "value_col   = 'Total revenue'\n",
    "\n",
    "sdf = complete.copy()\n",
    "\n",
    "# --- 1) Clean + parse dates robustly (handles 1/2-digit M/D and 2- or 4-digit years) ---\n",
    "sdf[date_col] = (\n",
    "    sdf[date_col]\n",
    "    .astype(str)\n",
    "    .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "    .str.replace(\"\\r\", \" \", regex=False)\n",
    "    .str.replace(\"\\n\", \" \", regex=False)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# first pass (flexible)\n",
    "parsed = pd.to_datetime(sdf[date_col], errors=\"coerce\")\n",
    "\n",
    "# fallback: expand 2-digit years like m/d/yy -> m/d/20yy (e.g., 9/5/25 -> 9/5/2025)\n",
    "mask_na = parsed.isna()\n",
    "if mask_na.any():\n",
    "    fixed_2yy = sdf.loc[mask_na, date_col].str.replace(\n",
    "        r\"^(\\d{1,2})/(\\d{1,2})/(\\d{2})$\",\n",
    "        r\"\\1/\\2/20\\3\",\n",
    "        regex=True\n",
    "    )\n",
    "    parsed.loc[mask_na] = pd.to_datetime(fixed_2yy, errors=\"coerce\")\n",
    "\n",
    "sdf[date_col] = parsed\n",
    "# keep only rows with a valid date\n",
    "sdf = sdf.dropna(subset=[date_col])\n",
    "\n",
    "# ensure revenue numeric\n",
    "sdf[value_col] = pd.to_numeric(sdf[value_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# optional: drop missing owner unit rows\n",
    "sdf = sdf.dropna(subset=[product_col])\n",
    "\n",
    "# --- 2) Quarter labels and completeness over time range ---\n",
    "sdf['quarter'] = sdf[date_col].dt.to_period('Q')\n",
    "\n",
    "# sum by quarter x owner unit\n",
    "rev_q_pl = (\n",
    "    sdf.groupby(['quarter', product_col], dropna=False)[value_col]\n",
    "       .sum()\n",
    "       .reset_index(name='revenue')\n",
    ")\n",
    "\n",
    "# per-quarter totals and percentages\n",
    "totals = rev_q_pl.groupby('quarter', as_index=False)['revenue'].sum().rename(columns={'revenue':'q_total'})\n",
    "rev_q_pl = rev_q_pl.merge(totals, on='quarter', how='left')\n",
    "rev_q_pl['pct'] = np.where(rev_q_pl['q_total'] > 0, 100 * rev_q_pl['revenue'] / rev_q_pl['q_total'], 0.0)\n",
    "\n",
    "# --- Build a complete PeriodIndex of quarters (unique) ---\n",
    "if not sdf.empty:\n",
    "    q_start = sdf['quarter'].min()\n",
    "    q_end   = sdf['quarter'].max()\n",
    "    all_quarters = pd.period_range(q_start, q_end, freq='Q')\n",
    "else:\n",
    "    all_quarters = pd.PeriodIndex([], freq='Q')\n",
    "\n",
    "# --- Keep top 3 owner units per quarter by percentage ---\n",
    "top3 = (\n",
    "    rev_q_pl.sort_values(['quarter', 'pct'], ascending=[True, False])\n",
    "            .groupby('quarter', as_index=False)\n",
    "            .head(3)\n",
    ")\n",
    "\n",
    "# === Build PERCENT stacks: top-3 % + \"Other\" so each bar reaches 100% ===\n",
    "pct_pivot = top3.pivot_table(\n",
    "    index='quarter',\n",
    "    columns=product_col,   # Owner unit\n",
    "    values='pct',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Ensure all quarters present (even if 0%)\n",
    "pct_pivot = pct_pivot.reindex(all_quarters, fill_value=0)\n",
    "\n",
    "# Add \"Other\" percentage per quarter to reach 100\n",
    "row_sum = pct_pivot.sum(axis=1)\n",
    "pct_pivot['Other'] = (100 - row_sum).clip(lower=0, upper=100)\n",
    "\n",
    "# Order stacks by global contribution; put \"Other\" last for readability\n",
    "col_order = pct_pivot.sum(axis=0).sort_values(ascending=False).index.tolist()\n",
    "if 'Other' in col_order:\n",
    "    col_order = [c for c in col_order if c != 'Other'] + ['Other']\n",
    "\n",
    "# --- Plot stacked bars in PERCENTAGES (left y-axis) ---\n",
    "quarters_sorted = list(pct_pivot.index.astype(str))\n",
    "\n",
    "spacing = 1.25\n",
    "x = np.arange(len(quarters_sorted)) * spacing\n",
    "bar_width = 0.8\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "# Stable colors by unit name; force \"Other\" to gray\n",
    "units = list(pct_pivot.columns)\n",
    "n_series = len(units)\n",
    "cmap = (plt.cm.get_cmap('tab20', n_series) if n_series <= 20 else plt.cm.get_cmap('hsv', n_series))\n",
    "unit_colors = {u: ((0.6, 0.6, 0.6, 1.0) if u == 'Other' else cmap(i)) for i, u in enumerate(units)}\n",
    "\n",
    "bottom = np.zeros(len(pct_pivot), dtype=float)\n",
    "for col in col_order:\n",
    "    vals = pct_pivot[col].values\n",
    "    if np.any(vals != 0):\n",
    "        ax.bar(x, vals, width=bar_width, bottom=bottom, label=col, color=unit_colors[col])\n",
    "        bottom += vals\n",
    "\n",
    "# Left y-axis: percentage\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_ylabel(\"Percent of Total Revenue\")\n",
    "\n",
    "# X-axis labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(quarters_sorted, rotation=45, ha=\"right\")\n",
    "ax.tick_params(axis=\"x\", labelsize=9, pad=8)\n",
    "plt.subplots_adjust(bottom=0.25)\n",
    "ax.margins(x=0.02)\n",
    "\n",
    "ax.set_title(\"Share of Revenue and Total Revenue per Quarter\")\n",
    "\n",
    "# --- Overlay line of quarter totals on RIGHT y-axis (dark navy) ---\n",
    "qtot = totals.set_index('quarter')['q_total'].reindex(all_quarters).fillna(0)\n",
    "ax2 = ax.twinx()\n",
    "line_plot, = ax2.plot(x, qtot.values, marker='o', linewidth=2, color='#0b1e3f', label=\"Quarter Total\")\n",
    "ax2.set_ylabel(\"Total Revenue (Billion USD)\")\n",
    "\n",
    "# Give extra right margin for the farther legend\n",
    "fig.subplots_adjust(right=0.78)\n",
    "\n",
    "# --- Combined legend (bars + line), moved further right ---\n",
    "handles1, labels1 = ax.get_legend_handles_labels()\n",
    "ax.legend(\n",
    "    handles1 + [line_plot],\n",
    "    labels1 + [\"Quarter Total\"],\n",
    "    title=\"BU\",\n",
    "    bbox_to_anchor=(1.05, 1.00),  # farther from plot\n",
    "    loc=\"upper left\",\n",
    "    frameon=False\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ongoing - major product lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "product_col = 'Product\\nline'\n",
    "value_col   = 'Total revenue'\n",
    "threshold_pct = 3.0  # group categories contributing < 3% into \"Other\"\n",
    "\n",
    "# --- 1) Prep & aggregate ---\n",
    "og = ongoing.copy()\n",
    "og[value_col] = pd.to_numeric(og[value_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# --- 1a) Map Product line -> Product group by prefix ---\n",
    "pl_norm = og[product_col].astype(str).str.strip().str.lower()\n",
    "\n",
    "is_event     = pl_norm.str.startswith('event')\n",
    "is_viewables = pl_norm.str.startswith('viewables')\n",
    "is_home      = pl_norm.str.startswith('home')\n",
    "is_wearable  = pl_norm.str.startswith('wearable')\n",
    "is_hearable  = pl_norm.str.startswith('hearable')\n",
    "is_others    = pl_norm.str.startswith('others')  # treat explicit \"others\" as Other\n",
    "\n",
    "og['Product group'] = np.select(\n",
    "    [is_event, is_viewables, is_home, is_wearable, is_hearable, is_others],\n",
    "    ['Event',  'Viewables',  'Home', 'Wearable', 'Hearable',  'Other'],\n",
    "    default='Other'  # fallback for anything else (incl. NaN after astype(str))\n",
    ")\n",
    "\n",
    "group_col = 'Product group'  # use groups for the pie\n",
    "\n",
    "# --- 1b) Aggregate by group (not by individual product lines) ---\n",
    "by_group = (\n",
    "    og.groupby(group_col, dropna=False)[value_col]\n",
    "      .sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "total_revenue = float(by_group.sum())\n",
    "print(f\"Total revenue (ongoing): {total_revenue:,.0f}\")\n",
    "\n",
    "if total_revenue == 0:\n",
    "    print(\"No revenue in 'ongoing' to plot.\")\n",
    "else:\n",
    "    # --- 2) Percentages & thresholding (avoid double 'Other') ---\n",
    "    pct = (by_group / total_revenue) * 100.0\n",
    "    base_other = float(by_group.get('Other', 0.0))\n",
    "\n",
    "    keep_mask = (pct >= threshold_pct) | (by_group.index == 'Other')\n",
    "    kept = by_group[keep_mask]\n",
    "    small_sum = by_group[~keep_mask].sum()\n",
    "\n",
    "    # Merge small categories into 'Other'\n",
    "    others_value = base_other + small_sum\n",
    "    final = kept.drop(index='Other', errors='ignore')\n",
    "    if others_value > 0:\n",
    "        final = pd.concat([final, pd.Series({'Other': others_value})])\n",
    "\n",
    "    # Ensure descending order for legend consistency\n",
    "    final = final.sort_values(ascending=False)\n",
    "\n",
    "    # --- 3) Unique colors (no reuse) ---\n",
    "    n = len(final)\n",
    "    cmap = plt.cm.get_cmap('tab20', n) if n <= 20 else plt.cm.get_cmap('hsv', n)\n",
    "    colors = [cmap(i) for i in range(n)]\n",
    "\n",
    "    # --- 3a) Two-line labels in millions; hide *label text* for 'Other'\n",
    "    group_names = list(final.index)\n",
    "    labels_two_line = [\n",
    "        (f\"{name}({val/1e6:.1f}M)\" if name != 'Other' else \"\")\n",
    "        for name, val in final.items()\n",
    "    ]\n",
    "\n",
    "    # --- 3b) autopct callable that *skips* percent for 'Other'\n",
    "    def autopct_excluding_other(names):\n",
    "        def fmt(pct):\n",
    "            fmt.i += 1\n",
    "            idx = fmt.i - 1\n",
    "            return \"\" if names[idx] == 'Other' else f\"{pct:.1f}%\"\n",
    "        fmt.i = 0\n",
    "        return fmt\n",
    "\n",
    "    # --- 4) Pie chart (original style, with 'Other' unlabeled) ---\n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        final.values,\n",
    "        labels=labels_two_line,                          # blank label for 'Other'\n",
    "        autopct=autopct_excluding_other(group_names),    # blank % for 'Other'\n",
    "        startangle=90,\n",
    "        counterclock=False,\n",
    "        colors=colors,\n",
    "        labeldistance=1.25,                              # small nudge outward\n",
    "        pctdistance=0.72,                                # % a bit toward center\n",
    "        textprops={\"fontsize\": 10}\n",
    "    )\n",
    "\n",
    "    ax.axis('equal')\n",
    "    ax.set_title(\"Potential Revenue by Main Product Lines\")\n",
    "\n",
    "    # --- 5) Legend: include ALL groups (including 'Other') ---\n",
    "    plt.subplots_adjust(right=0.58)  # leave more empty space on the right\n",
    "    ax.legend(\n",
    "        handles=wedges,\n",
    "        labels=list(final.index),     # include 'Other' in legend\n",
    "        title=\"Product Line\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.50, 0.5),   # move legend farther from the axes\n",
    "        frameon=True,\n",
    "        borderaxespad=0.0\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ongoing - specific product lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "product_col = 'Product\\nline'\n",
    "value_col   = 'Total revenue'\n",
    "threshold_pct = 3.0  # group categories contributing < 3% into \"Other\"\n",
    "\n",
    "# --- 1) Prep & aggregate ---\n",
    "og = ongoing.copy()\n",
    "og[value_col] = pd.to_numeric(og[value_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "by_pl = (\n",
    "    og.groupby(product_col, dropna=False)[value_col]\n",
    "      .sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "by_pl.index = by_pl.index.fillna(\"Unknown\")\n",
    "\n",
    "total_revenue = float(by_pl.sum())\n",
    "print(f\"Total revenue (ongoing): {total_revenue:,.0f}\")\n",
    "\n",
    "if total_revenue == 0:\n",
    "    print(\"No revenue in 'ongoing' to plot.\")\n",
    "else:\n",
    "    # --- 2) Percentages & thresholding ---\n",
    "    pct = (by_pl / total_revenue) * 100.0\n",
    "    keep_mask = pct >= threshold_pct\n",
    "    kept = by_pl[keep_mask]\n",
    "    others_value = by_pl[~keep_mask].sum()\n",
    "\n",
    "    # Build final series with \"Other\" group if needed\n",
    "    if others_value > 0:\n",
    "        final = pd.concat([kept, pd.Series({\"Other\": others_value})])\n",
    "    else:\n",
    "        final = kept.copy()\n",
    "\n",
    "    # Ensure descending order for legend consistency\n",
    "    final = final.sort_values(ascending=False)\n",
    "\n",
    "    # --- 3) Unique colors (no reuse) ---\n",
    "    n = len(final)\n",
    "    cmap = plt.cm.get_cmap('tab20', n) if n <= 20 else plt.cm.get_cmap('hsv', n)\n",
    "    colors = [cmap(i) for i in range(n)]\n",
    "\n",
    "    # Build labels as \"Name (xx.xM)\" where values are in millions\n",
    "    labels_with_values = [f\"{name} ({val/1e6:.1f}M)\" for name, val in final.items()]\n",
    "\n",
    "    # --- 4) Pie chart ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        final.values,\n",
    "        labels=labels_with_values,          \n",
    "        autopct=lambda p: f\"{p:.1f}%\",\n",
    "        startangle=90,\n",
    "        counterclock=False,\n",
    "        colors=colors\n",
    "    )\n",
    "\n",
    "    ax.axis('equal')\n",
    "    ax.set_title(\"Potential Revenue by Specific Product Lines\")\n",
    "\n",
    "    # --- 5) Legend: ONLY color and product line name ---\n",
    "    plt.subplots_adjust(right=0.58)  # leave more empty space on the right\n",
    "    ax.legend(\n",
    "        handles=wedges,\n",
    "        labels=list(final.index),     # only color + product line name\n",
    "        title=\"Product line\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.70, 0.5),  # move legend farther from the axes (increase to push further)\n",
    "        frameon=True,\n",
    "        borderaxespad=0.0\n",
    "    )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ongoing - BUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "group_col = 'Owner unit'      # grouping column\n",
    "value_col = 'Total revenue'   # numeric values\n",
    "threshold_pct = 3.0           # group categories contributing < 3% into \"Other\"\n",
    "\n",
    "# --- 1) Prep & aggregate ---\n",
    "og = ongoing.copy()\n",
    "og[value_col] = pd.to_numeric(og[value_col], errors=\"coerce\").fillna(0)\n",
    "\n",
    "by_group = (\n",
    "    og.groupby(group_col, dropna=False)[value_col]\n",
    "      .sum()\n",
    "      .sort_values(ascending=False)\n",
    ")\n",
    "by_group.index = by_group.index.fillna(\"Unknown\")\n",
    "\n",
    "total_revenue = float(by_group.sum())\n",
    "print(f\"Total revenue (ongoing): {total_revenue:,.0f}\")\n",
    "\n",
    "if total_revenue == 0:\n",
    "    print(\"No revenue in 'ongoing' to plot.\")\n",
    "else:\n",
    "    # --- 2) Percentages & thresholding ---\n",
    "    pct = (by_group / total_revenue) * 100.0\n",
    "    keep_mask = pct >= threshold_pct\n",
    "    kept = by_group[keep_mask]\n",
    "    others_value = by_group[~keep_mask].sum()\n",
    "\n",
    "    # Build final series with \"Other\" bucket if needed\n",
    "    if others_value > 0:\n",
    "        final = pd.concat([kept, pd.Series({\"Other\": others_value})])\n",
    "    else:\n",
    "        final = kept.copy()\n",
    "\n",
    "    # Order for consistent legend appearance\n",
    "    final = final.sort_values(ascending=False)\n",
    "\n",
    "    # --- 3) Unique colors (no reuse) ---\n",
    "    n = len(final)\n",
    "    cmap = plt.cm.get_cmap('tab20', n) if n <= 20 else plt.cm.get_cmap('hsv', n)\n",
    "    colors = [cmap(i) for i in range(n)]\n",
    "\n",
    "    # Build labels as \"Name (xx.xM)\" where values are in millions\n",
    "    labels_with_values = [f\"{name} ({val/1e6:.1f}M)\" for name, val in final.items()]\n",
    "\n",
    "    # --- 4) Pie chart (labels spaced farther apart) ---\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        final.values,\n",
    "        labels=labels_with_values,          # â† use name + total revenue in millions\n",
    "        autopct=lambda p: f\"{p:.1f}%\",\n",
    "        startangle=90,\n",
    "        counterclock=False,\n",
    "        colors=colors,\n",
    "        labeldistance=1.3,\n",
    "        pctdistance=0.7,\n",
    "        textprops={\"fontsize\": 10}\n",
    "    )\n",
    "\n",
    "    ax.axis('equal')\n",
    "    ax.set_title(\"Potential Revenue by Business Units\")\n",
    "\n",
    "    # --- 5) Legend: ONLY color + name, placed farther from the pie ---\n",
    "    plt.subplots_adjust(right=0.58)  # reserve more space on the right\n",
    "    ax.legend(\n",
    "        handles=wedges,\n",
    "        labels=list(final.index),\n",
    "        title=\"BU\",\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.45, 0.5),\n",
    "        frameon=True,\n",
    "        borderaxespad=0.0\n",
    "    )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust numeric parser (same as before)\n",
    "def to_number(s: pd.Series) -> pd.Series:\n",
    "    t = s.astype(str)\n",
    "    t = t.str.replace(\"\\u00a0\", \" \", regex=False)     # NBSP -> space\n",
    "    t = t.str.replace(\",\", \"\", regex=False)           # remove commas\n",
    "    t = t.str.replace(\"$\", \"\", regex=False)           # remove $\n",
    "    t = t.str.replace(r\"^\\((.*)\\)$\", r\"-\\1\", regex=True)  # (123) -> -123\n",
    "    # extract first numeric token and convert\n",
    "    return pd.to_numeric(t.str.extract(r\"(-?\\d+(?:\\.\\d+)?)\", expand=False), errors=\"coerce\")\n",
    "\n",
    "# --- Parse numbers from sources on the WHOLE tracker ---\n",
    "declared_num = to_number(suspended[declared])\n",
    "derived_num  = to_number(suspended[derived])\n",
    "\n",
    "# 1) Mask-level equivalence (your test)\n",
    "mask_sources_nonzero = (declared_num.fillna(0) != 0) | (derived_num.fillna(0) != 0)\n",
    "mask_total_nonzero   = suspended[\"Total revenue\"].fillna(0) != 0\n",
    "assert (mask_sources_nonzero == mask_total_nonzero).all(), \"Mismatch: non-zero masks differ\"\n",
    "\n",
    "# 2) Value-level equivalence (exact numbers with your precedence rule)\n",
    "expected = np.where(declared_num.notna() & (declared_num != 0), declared_num,\n",
    "            np.where(derived_num.notna()  & (derived_num  != 0), derived_num, 0))\n",
    "expected = pd.Series(expected, index=suspended.index).fillna(0).astype(int)\n",
    "assert suspended[\"Total revenue\"].equals(expected), \"Mismatch: values differ from precedence rule\"\n",
    "print(\"Diagnostics passed: masks and values align.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Opportunity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Columns ---\n",
    "prod_col    = 'Product\\nline'\n",
    "time_col = 'Created'\n",
    "stage_col   = 'Stage'\n",
    "\n",
    "# --- 1) Make a working copy ---\n",
    "df = tracker.copy()\n",
    "\n",
    "# --- 2) Map Product line to top-level group by prefix ---\n",
    "# Normalize for prefix checks\n",
    "pl = df[prod_col].astype(str).str.strip().str.lower()\n",
    "\n",
    "group = np.select(\n",
    "    [\n",
    "        pl.str.startswith('event'),\n",
    "        pl.str.startswith('viewables'),\n",
    "        pl.str.startswith('home'),\n",
    "        pl.str.startswith('others'),\n",
    "        pl.str.startswith('wearable'),\n",
    "        pl.str.startswith('hearable'),\n",
    "    ],\n",
    "    ['Event', 'Viewables', 'Home', 'Others', 'Wearable', 'Hearable'],\n",
    "    default='Other'  # fallback if no known prefix\n",
    ")\n",
    "df['Product group'] = group\n",
    "\n",
    "# (Optional) if you want to drop 'Other' (fallbacks), uncomment:\n",
    "# df = df[df['Product group'] != 'Other']\n",
    "\n",
    "# --- 3) Parse Created -> datetime robustly, then to quarter ---\n",
    "clean_created = (\n",
    "    df[time_col].astype(str)\n",
    "      .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "      .str.replace(\"\\r\", \" \", regex=False)\n",
    "      .str.replace(\"\\n\", \" \", regex=False)\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "parsed = pd.to_datetime(clean_created, errors='coerce')  # flexible first pass\n",
    "\n",
    "# Fallback: convert m/d/yy to m/d/20yy (e.g., 9/5/25 -> 9/5/2025)\n",
    "mask_na = parsed.isna()\n",
    "if mask_na.any():\n",
    "    fixed_2yy = clean_created[mask_na].str.replace(\n",
    "        r\"^(\\d{1,2})/(\\d{1,2})/(\\d{2})$\",\n",
    "        r\"\\1/\\2/20\\3\",\n",
    "        regex=True\n",
    "    )\n",
    "    parsed.loc[mask_na] = pd.to_datetime(fixed_2yy, errors='coerce')\n",
    "\n",
    "df['Created_dt'] = parsed\n",
    "df = df.dropna(subset=['Created_dt'])  # keep rows with valid dates\n",
    "df['quarter'] = df['Created_dt'].dt.to_period('Q')\n",
    "\n",
    "# --- 4) Filter to rows where Stage contains \"opportunity\" (case-insensitive) ---\n",
    "is_oppty = df[stage_col].astype(str).str.contains('opportunity', case=False, na=False)\n",
    "df_oppty = df.loc[is_oppty, ['quarter', 'Product group']].copy()\n",
    "\n",
    "# --- 5) Count opportunities per quarter x product group ---\n",
    "counts = (\n",
    "    df_oppty.groupby(['quarter', 'Product group'])\n",
    "            .size()\n",
    "            .unstack('Product group', fill_value=0)\n",
    ")\n",
    "\n",
    "# Ensure continuous quarters on the index (even if zero)\n",
    "if not counts.empty:\n",
    "    all_quarters = pd.period_range(counts.index.min(), counts.index.max(), freq='Q')\n",
    "    counts = counts.reindex(all_quarters, fill_value=0)\n",
    "\n",
    "# Optional: order columns by total volume (largest first)\n",
    "col_order = counts.sum(axis=0).sort_values(ascending=False).index.tolist()\n",
    "counts = counts[col_order]\n",
    "\n",
    "# --- 6) Plot: lines per product group over quarters ---\n",
    "quarters = list(counts.index.astype(str))\n",
    "x = np.arange(len(quarters))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Use a categorical colormap with enough distinct colors\n",
    "n_series = counts.shape[1]\n",
    "cmap = plt.cm.get_cmap('tab20', n_series) if n_series <= 20 else plt.cm.get_cmap('hsv', n_series)\n",
    "colors = [cmap(i) for i in range(n_series)]\n",
    "\n",
    "line_width = 3\n",
    "marker_size = 6\n",
    "\n",
    "for i, col in enumerate(counts.columns):\n",
    "    ax.plot(\n",
    "        x,\n",
    "        counts[col].values,\n",
    "        label=col,\n",
    "        linewidth=line_width,   # thicker\n",
    "        marker='o',\n",
    "        markersize=marker_size,\n",
    "        color=colors[i]\n",
    "    )\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(quarters, rotation=45, ha='right')\n",
    "ax.set_ylabel('Number of opportunities')\n",
    "ax.set_title('Quarterly Opportunities by Main Product Lines')\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "# Force integer y-axis ticks with step = 1\n",
    "y_max = int(np.nanmax(counts.values)) if counts.size else 0\n",
    "ax.set_ylim(0, max(1, y_max))\n",
    "ax.yaxis.set_major_locator(MultipleLocator(1))\n",
    "ax.margins(x=0.02)\n",
    "\n",
    "# Legend outside (optional). Comment these two lines to keep legend inside.\n",
    "plt.subplots_adjust(right=0.82)\n",
    "ax.legend(title='Product Line', loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Time Spent on Suspended Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Columns ---\n",
    "product_col = 'Product\\nline'\n",
    "stage_col   = 'Stage'\n",
    "time_col    = 'Time\\n(days)'\n",
    "\n",
    "# --- 1) Copy & basic cleaning ---\n",
    "df = suspended.copy()\n",
    "df[time_col] = pd.to_numeric(df[time_col], errors='coerce')  # ensure numeric\n",
    "\n",
    "# --- 2) Filter to Stage contains \"opportunity\" (case-insensitive) ---\n",
    "is_oppty = df[stage_col].astype(str).str.contains('opportunity', case=False, na=False)\n",
    "df_oppty = df.loc[is_oppty].copy()\n",
    "\n",
    "if df_oppty.empty:\n",
    "    print(\"No rows in 'suspended' with Stage containing 'opportunity'.\")\n",
    "else:\n",
    "    # --- 3) Map Product line -> Product group by prefix ---\n",
    "    pl_norm = df_oppty[product_col].astype(str).str.strip().str.lower()\n",
    "\n",
    "    is_event     = pl_norm.str.startswith('event')\n",
    "    is_viewables = pl_norm.str.startswith('viewables')\n",
    "    is_home      = pl_norm.str.startswith('home')\n",
    "    is_wearable  = pl_norm.str.startswith('wearable')\n",
    "    is_hearable  = pl_norm.str.startswith('hearable')\n",
    "    is_others    = pl_norm.str.startswith('others')  # explicit \"others\"\n",
    "\n",
    "    df_oppty['Product group'] = np.select(\n",
    "        [is_event, is_viewables, is_home, is_wearable, is_hearable, is_others],\n",
    "        ['Event',  'Viewables',  'Home', 'Wearable', 'Hearable',  'Other'],\n",
    "        default='Other'  # fallback for anything else (incl. NaN after astype(str))\n",
    "    )\n",
    "\n",
    "    # --- 4) Compute average Time(days) per Product group ---\n",
    "    group_means = (\n",
    "        df_oppty.groupby('Product group', dropna=False)[time_col]\n",
    "                .mean()\n",
    "                .dropna()\n",
    "                .sort_values(ascending=False)\n",
    "    )\n",
    "\n",
    "    if group_means.empty:\n",
    "        print(\"No valid 'Time\\\\n(days)' values after filtering.\")\n",
    "    else:\n",
    "        # --- 5) Bar plot (horizontal) ---\n",
    "        y = np.arange(len(group_means))\n",
    "        vals = group_means.values\n",
    "        labels = group_means.index.tolist()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.barh(y, vals)                 # no custom colors per your style constraints\n",
    "        ax.set_yticks(y)\n",
    "        ax.set_yticklabels(labels)\n",
    "        ax.invert_yaxis()                # largest at top\n",
    "\n",
    "        ax.set_xlabel(\"Number of Days\")\n",
    "        ax.set_title(\"Average Time Spent on Opportunities by Main Product Lines\")\n",
    "\n",
    "        # Optional: annotate values on bars\n",
    "        for yi, v in zip(y, vals):\n",
    "            ax.text(v, yi, f\"  {v:.1f}\", va=\"center\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Time\\n(days)'\n",
    "prod_col = 'Product\\nline'\n",
    "\n",
    "# numeric series for sorting\n",
    "s = pd.to_numeric(tracker[col], errors='coerce')\n",
    "\n",
    "# top 5 rows by value (keeps duplicates)\n",
    "top5_idx = s.nlargest(5).index\n",
    "\n",
    "# grab corresponding product lines + numeric time\n",
    "top5 = tracker.loc[top5_idx, [prod_col]].copy()\n",
    "top5['Time (days)'] = s.loc[top5_idx].values  # ensure numeric\n",
    "\n",
    "# optional: keep the exact original order (already descending)\n",
    "print(top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quote Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Product Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Columns ---\n",
    "prod_col     = 'Product\\nline'\n",
    "time_col  = 'Updated'\n",
    "flag_cols    = ['RFI', 'RFP', 'RFQ', 'SOW']\n",
    "\n",
    "# --- 1) Working copy ---\n",
    "df = tracker.copy()\n",
    "\n",
    "# --- 2) Map Product line to 6 top-level groups by prefix ---\n",
    "pl_norm = df[prod_col].astype(str).str.strip().str.lower()\n",
    "groups = np.select(\n",
    "    [\n",
    "        pl_norm.str.startswith('event'),\n",
    "        pl_norm.str.startswith('viewables'),\n",
    "        pl_norm.str.startswith('home'),\n",
    "        pl_norm.str.startswith('others'),\n",
    "        pl_norm.str.startswith('wearable'),\n",
    "        pl_norm.str.startswith('hearable'),\n",
    "    ],\n",
    "    ['Event', 'Viewables', 'Home', 'Others', 'Wearable', 'Hearable'],\n",
    "    default=np.nan  # drop anything that doesn't match these 6 prefixes\n",
    ")\n",
    "df['Product group'] = groups\n",
    "df = df.dropna(subset=['Product group'])\n",
    "\n",
    "# --- 3) Parse Created -> datetime robustly, then to quarter ---\n",
    "time_clean = (\n",
    "    df[time_col].astype(str)\n",
    "      .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "      .str.replace(\"\\r\", \" \", regex=False)\n",
    "      .str.replace(\"\\n\", \" \", regex=False)\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "created_dt = pd.to_datetime(time_clean, errors='coerce')\n",
    "mask_na = created_dt.isna()\n",
    "if mask_na.any():\n",
    "    # fallback: m/d/yy -> m/d/20yy\n",
    "    fixed_2yy = time_clean[mask_na].str.replace(\n",
    "        r\"^(\\d{1,2})/(\\d{1,2})/(\\d{2})$\",\n",
    "        r\"\\1/\\2/20\\3\",\n",
    "        regex=True\n",
    "    )\n",
    "    created_dt.loc[mask_na] = pd.to_datetime(fixed_2yy, errors='coerce')\n",
    "\n",
    "df['Created_dt'] = created_dt\n",
    "df = df.dropna(subset=['Created_dt'])\n",
    "df['quarter'] = df['Created_dt'].dt.to_period('Q')\n",
    "\n",
    "# --- 4) Keep rows where at least one of the flags contains \"Yes\" (case-insensitive) ---\n",
    "present_flags = [c for c in flag_cols if c in df.columns]\n",
    "if not present_flags:\n",
    "    raise KeyError(f\"None of the flag columns are present: {flag_cols}\")\n",
    "\n",
    "yes_any = (\n",
    "    df[present_flags]\n",
    "      .astype(str)\n",
    "      .apply(lambda col: col.str.contains(r'\\byes\\b', case=False, na=False))\n",
    "      .any(axis=1)\n",
    ")\n",
    "df_yes = df.loc[yes_any, ['quarter', 'Product group']].copy()\n",
    "\n",
    "# --- 5) Count per quarter x product group ---\n",
    "counts = (\n",
    "    df_yes.groupby(['quarter', 'Product group'])\n",
    "          .size()\n",
    "          .unstack('Product group', fill_value=0)\n",
    ")\n",
    "\n",
    "# Ensure continuous quarter index (even if no rows in some quarters)\n",
    "if not counts.empty:\n",
    "    all_quarters = pd.period_range(counts.index.min(), counts.index.max(), freq='Q')\n",
    "    counts = counts.reindex(all_quarters, fill_value=0)\n",
    "\n",
    "# Order groups by total volume (largest first) for a tidy legend\n",
    "col_order = counts.sum(axis=0).sort_values(ascending=False).index.tolist()\n",
    "counts = counts[col_order]\n",
    "\n",
    "# --- 6) Plot: lines per product group over quarters ---\n",
    "quarters = list(counts.index.astype(str))\n",
    "x = np.arange(len(quarters))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# distinct colors; thicker lines\n",
    "n_series = counts.shape[1]\n",
    "cmap = plt.cm.get_cmap('tab20', n_series) if n_series <= 20 else plt.cm.get_cmap('hsv', n_series)\n",
    "colors = [cmap(i) for i in range(n_series)]\n",
    "\n",
    "line_width = 3\n",
    "marker_size = 6\n",
    "for i, col in enumerate(counts.columns):\n",
    "    ax.plot(\n",
    "        x,\n",
    "        counts[col].values,\n",
    "        label=col,\n",
    "        linewidth=line_width,\n",
    "        marker='o',\n",
    "        markersize=marker_size,\n",
    "        color=colors[i]\n",
    "    )\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(quarters, rotation=45, ha='right')\n",
    "ax.set_ylabel('Number of Cases with A Quote')\n",
    "ax.set_title('RFI/RFP/RFQ/SOW per Quarter by Main Product Lines')\n",
    "ax.grid(True, axis='y', alpha=0.3)\n",
    "ax.margins(x=0.02)\n",
    "\n",
    "# Legend outside (optional)\n",
    "plt.subplots_adjust(right=0.82, bottom=0.18)\n",
    "ax.legend(title='Product Line', loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=True)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Columns ---\n",
    "owner_col   = \"Owner unit\"\n",
    "updated_col = \"Updated\"\n",
    "flag_cols   = [\"RFI\", \"RFP\", \"RFQ\", \"SOW\"]\n",
    "\n",
    "# --- 1) Working copy & robust date parsing for Updated -> quarter ---\n",
    "df = tracker.copy()\n",
    "\n",
    "clean_upd = (\n",
    "    df[updated_col].astype(str)\n",
    "      .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "      .str.replace(\"\\r\", \" \", regex=False)\n",
    "      .str.replace(\"\\n\", \" \", regex=False)\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "upd_dt = pd.to_datetime(clean_upd, errors=\"coerce\")\n",
    "\n",
    "# Fallback: m/d/yy -> m/d/20yy (e.g., 9/5/25 -> 9/5/2025)\n",
    "mask_na = upd_dt.isna()\n",
    "if mask_na.any():\n",
    "    fixed_2yy = clean_upd[mask_na].str.replace(\n",
    "        r\"^(\\d{1,2})/(\\d{1,2})/(\\d{2})$\",\n",
    "        r\"\\1/\\2/20\\3\",\n",
    "        regex=True\n",
    "    )\n",
    "    upd_dt.loc[mask_na] = pd.to_datetime(fixed_2yy, errors=\"coerce\")\n",
    "\n",
    "df[\"Updated_dt\"] = upd_dt\n",
    "df = df.dropna(subset=[\"Updated_dt\"])\n",
    "df[\"quarter\"] = df[\"Updated_dt\"].dt.to_period(\"Q\")\n",
    "\n",
    "# --- 2) Keep rows where at least one of the flag columns contains \"Yes\" ---\n",
    "present_flags = [c for c in flag_cols if c in df.columns]\n",
    "if not present_flags:\n",
    "    raise KeyError(f\"None of the flag columns are present: {flag_cols}\")\n",
    "\n",
    "yes_any = (\n",
    "    df[present_flags]\n",
    "      .astype(str)\n",
    "      .apply(lambda col: col.str.contains(r\"\\byes\\b\", case=False, na=False))\n",
    "      .any(axis=1)\n",
    ")\n",
    "df_yes = df.loc[yes_any, [\"quarter\", owner_col]].copy()\n",
    "df_yes[owner_col] = df_yes[owner_col].fillna(\"Unknown\")\n",
    "\n",
    "# --- 3) Counts per quarter x owner; totals & percentages per quarter ---\n",
    "counts = (\n",
    "    df_yes.groupby([\"quarter\", owner_col])\n",
    "          .size()\n",
    "          .reset_index(name=\"n\")\n",
    ")\n",
    "\n",
    "# Ensure continuous quarter range (even if 0)\n",
    "all_quarters = pd.period_range(df[\"quarter\"].min(), df[\"quarter\"].max(), freq=\"Q\")\n",
    "\n",
    "totals = counts.groupby(\"quarter\", as_index=False)[\"n\"].sum().rename(columns={\"n\": \"q_total\"})\n",
    "pct = counts.merge(totals, on=\"quarter\", how=\"left\")\n",
    "pct[\"pct\"] = np.where(pct[\"q_total\"] > 0, 100 * pct[\"n\"] / pct[\"q_total\"], 0.0)\n",
    "\n",
    "# --- 4) Per quarter: keep top 3 Owner units, aggregate remaining to \"Other\" ---\n",
    "top3 = (\n",
    "    pct.sort_values([\"quarter\", \"pct\"], ascending=[True, False])\n",
    "       .groupby(\"quarter\", as_index=False)\n",
    "       .head(3)\n",
    ")\n",
    "\n",
    "top3_sum = top3.groupby(\"quarter\", as_index=False)[\"pct\"].sum().rename(columns={\"pct\": \"top3_pct\"})\n",
    "other = totals.merge(top3_sum, on=\"quarter\", how=\"left\").fillna({\"top3_pct\": 0})\n",
    "other[\"pct\"] = (100 - other[\"top3_pct\"]).clip(lower=0)\n",
    "other = other.loc[other[\"pct\"] > 0, [\"quarter\", \"pct\"]]\n",
    "other[owner_col] = \"Other\"\n",
    "\n",
    "plot_df = pd.concat(\n",
    "    [\n",
    "        top3[[\"quarter\", owner_col, \"pct\"]],\n",
    "        other[[\"quarter\", owner_col, \"pct\"]],\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# --- 5) Pivot to stacked % per quarter ---\n",
    "pivot = plot_df.pivot_table(\n",
    "    index=\"quarter\",\n",
    "    columns=owner_col,\n",
    "    values=\"pct\",\n",
    "    aggfunc=\"sum\",\n",
    "    fill_value=0,\n",
    ")\n",
    "\n",
    "pivot = pivot.reindex(all_quarters, fill_value=0)\n",
    "\n",
    "# Put \"Other\" last; sort remaining by total contribution\n",
    "cols = list(pivot.columns)\n",
    "if \"Other\" in cols:\n",
    "    non_other = [c for c in cols if c != \"Other\"]\n",
    "    non_other_sorted = pivot[non_other].sum().sort_values(ascending=False).index.tolist()\n",
    "    col_order = non_other_sorted + [\"Other\"]\n",
    "else:\n",
    "    col_order = pivot.sum().sort_values(ascending=False).index.tolist()\n",
    "pivot = pivot[col_order]\n",
    "\n",
    "# --- 6) Plot: stacked bars with UNIQUE colors per Owner unit (and matching legend) ---\n",
    "quarters = list(pivot.index.astype(str))\n",
    "x = np.arange(len(quarters))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Create a stable unique color for each Owner unit (no reuse)\n",
    "units = list(pivot.columns)\n",
    "n_series = len(units)\n",
    "cmap = plt.cm.get_cmap(\"tab20\", n_series) if n_series <= 20 else plt.cm.get_cmap(\"hsv\", n_series)\n",
    "unit_colors = {u: cmap(i) for i, u in enumerate(units)}\n",
    "\n",
    "bottom = np.zeros(len(pivot), dtype=float)\n",
    "legend_handles = []\n",
    "for u in units:\n",
    "    vals = pivot[u].values\n",
    "    bars = ax.bar(x, vals, bottom=bottom, label=u, color=unit_colors[u])\n",
    "    bottom += vals\n",
    "    # use the first bar from this series for the legend, so colors match exactly\n",
    "    legend_handles.append(bars[0])\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(quarters, rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Percent of Cases with A Quote\")\n",
    "ax.set_title('Top 3 Business Units in Percentage per Quarter')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "ax.margins(x=0.02)\n",
    "\n",
    "# Legend outside; uses the exact colored handles (no color reuse)\n",
    "plt.subplots_adjust(right=0.82, bottom=0.2)\n",
    "ax.legend(handles=legend_handles, labels=units, title=\"BU\",\n",
    "          loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=True)\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Status Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Columns ---\n",
    "updated_col = \"Updated\"\n",
    "status_col  = \"Status & health\"\n",
    "acct_col    = \"Account\"\n",
    "\n",
    "# --- 1) Working copy + robust date parsing (Updated â†’ quarter) ---\n",
    "df = tracker.copy()\n",
    "\n",
    "upd_clean = (\n",
    "    df[updated_col].astype(str)\n",
    "      .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "      .str.replace(\"\\r\", \" \", regex=False)\n",
    "      .str.replace(\"\\n\", \" \", regex=False)\n",
    "      .str.strip()\n",
    ")\n",
    "\n",
    "upd_dt = pd.to_datetime(upd_clean, errors=\"coerce\")\n",
    "mask_na = upd_dt.isna()\n",
    "if mask_na.any():\n",
    "    # fallback: m/d/yy -> m/d/20yy (e.g., 9/5/25 -> 9/5/2025)\n",
    "    fixed_2yy = upd_clean[mask_na].str.replace(\n",
    "        r\"^(\\d{1,2})/(\\d{1,2})/(\\d{2})$\",\n",
    "        r\"\\1/\\2/20\\3\",\n",
    "        regex=True\n",
    "    )\n",
    "    upd_dt.loc[mask_na] = pd.to_datetime(fixed_2yy, errors=\"coerce\")\n",
    "\n",
    "df[\"Updated_dt\"] = upd_dt\n",
    "df = df.dropna(subset=[\"Updated_dt\"])\n",
    "df[\"quarter\"] = df[\"Updated_dt\"].dt.to_period(\"Q\")\n",
    "\n",
    "# --- 2) Keep rows where Status & health is COMPLETE / COMPLETED ---\n",
    "is_complete = df[status_col].astype(str).str.fullmatch(r\"complete(d)?\", case=False, na=False)\n",
    "dfc = df.loc[is_complete, [\"quarter\", acct_col]].copy()\n",
    "dfc[acct_col] = dfc[acct_col].fillna(\"Unknown\")\n",
    "\n",
    "# --- 2a) Lump INT, INP, INP/U30 into single category \"INT/INP/U30\" ---\n",
    "acct_norm = dfc[acct_col].astype(str).str.strip()\n",
    "mask_combo = acct_norm.str.upper().isin({\"INT\", \"INT-INP\"})\n",
    "dfc.loc[mask_combo, acct_col] = \"INT/INP\"\n",
    "\n",
    "if dfc.empty:\n",
    "    print(\"No COMPLETE rows found.\")\n",
    "else:\n",
    "    # --- 3) Count COMPLETE rows per quarter Ã— account (with combined bucket) ---\n",
    "    counts = (\n",
    "        dfc.groupby([\"quarter\", acct_col])\n",
    "           .size()\n",
    "           .reset_index(name=\"n\")\n",
    "    )\n",
    "\n",
    "    # Continuous quarter index (based on COMPLETE rows)\n",
    "    all_quarters = pd.period_range(counts[\"quarter\"].min(), counts[\"quarter\"].max(), freq=\"Q\")\n",
    "\n",
    "    # Totals per quarter\n",
    "    totals = counts.groupby(\"quarter\", as_index=False)[\"n\"].sum().rename(columns={\"n\": \"q_total\"})\n",
    "\n",
    "    # --- 4) Per quarter: keep top 3 accounts, group the rest as \"Other\" ---\n",
    "    top3 = (\n",
    "        counts.sort_values([\"quarter\", \"n\"], ascending=[True, False])\n",
    "              .groupby(\"quarter\", as_index=False)\n",
    "              .head(3)\n",
    "    )\n",
    "\n",
    "    top3_sum = top3.groupby(\"quarter\", as_index=False)[\"n\"].sum().rename(columns={\"n\": \"top3_n\"})\n",
    "    other = totals.merge(top3_sum, on=\"quarter\", how=\"left\").fillna({\"top3_n\": 0})\n",
    "    other[\"n\"] = (other[\"q_total\"] - other[\"top3_n\"]).clip(lower=0)\n",
    "    other = other.loc[other[\"n\"] > 0, [\"quarter\", \"n\"]]\n",
    "    other[acct_col] = \"Other\"\n",
    "\n",
    "    plot_df = pd.concat(\n",
    "        [top3[[\"quarter\", acct_col, \"n\"]], other[[\"quarter\", acct_col, \"n\"]]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    # --- 5) Pivot to quarter Ã— (top3 accounts + Other) in COUNTS ---\n",
    "    pivot = plot_df.pivot_table(\n",
    "        index=\"quarter\",\n",
    "        columns=acct_col,\n",
    "        values=\"n\",\n",
    "        aggfunc=\"sum\",\n",
    "        fill_value=0\n",
    "    ).reindex(all_quarters, fill_value=0)\n",
    "\n",
    "    # Put \"Other\" last; sort remaining accounts by total count\n",
    "    cols = list(pivot.columns)\n",
    "    if \"Other\" in cols:\n",
    "        non_other = [c for c in cols if c != \"Other\"]\n",
    "        non_other_sorted = pivot[non_other].sum().sort_values(ascending=False).index.tolist()\n",
    "        col_order = non_other_sorted + [\"Other\"]\n",
    "    else:\n",
    "        col_order = pivot.sum().sort_values(ascending=False).index.tolist()\n",
    "    pivot = pivot[col_order]\n",
    "\n",
    "    # --- 6) Stacked bar plot (counts) ---\n",
    "    quarters = list(pivot.index.astype(str))\n",
    "    x = np.arange(len(quarters))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    bottom = np.zeros(len(pivot), dtype=float)\n",
    "\n",
    "    # stable unique colors per account\n",
    "    accounts = list(pivot.columns)\n",
    "    n_series = len(accounts)\n",
    "    cmap = plt.cm.get_cmap(\"tab20\", n_series) if n_series <= 20 else plt.cm.get_cmap(\"hsv\", n_series)\n",
    "    acct_colors = {a: cmap(i) for i, a in enumerate(accounts)}\n",
    "\n",
    "    handles = []\n",
    "    for a in accounts:\n",
    "        vals = pivot[a].values\n",
    "        bars = ax.bar(x, vals, bottom=bottom, label=a, color=acct_colors[a])\n",
    "        bottom += vals\n",
    "        handles.append(bars[0])\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(quarters, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Number of Complete Cases\")\n",
    "    ax.set_title('Top 3 Accounts by Number of Complete Cases per Quarter')\n",
    "    ax.grid(True, axis=\"y\", alpha=0.3)\n",
    "    ax.margins(x=0.02)\n",
    "\n",
    "    # Legend outside\n",
    "    plt.subplots_adjust(right=0.82, bottom=0.2)\n",
    "    ax.legend(handles=handles, labels=accounts, title=\"Account\",\n",
    "              loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=True)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Product Lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Columns ---\n",
    "prod_col    = 'Product\\nline'\n",
    "updated_col = 'Updated'\n",
    "status_col  = 'Status & health'\n",
    "\n",
    "# --- 1) Working copy ---\n",
    "df = tracker.copy()\n",
    "\n",
    "# --- 2) Map Product line to 6 top-level groups by prefix ---\n",
    "pl_norm = df[prod_col].astype(str).str.strip().str.lower()\n",
    "df['Product group'] = np.select(\n",
    "    [\n",
    "        pl_norm.str.startswith('event'),\n",
    "        pl_norm.str.startswith('viewables'),\n",
    "        pl_norm.str.startswith('home'),\n",
    "        pl_norm.str.startswith('others'),\n",
    "        pl_norm.str.startswith('wearable'),\n",
    "        pl_norm.str.startswith('hearable'),\n",
    "    ],\n",
    "    ['Event', 'Viewables', 'Home', 'Others', 'Wearable', 'Hearable'],\n",
    "    default=np.nan\n",
    ")\n",
    "df = df.dropna(subset=['Product group'])\n",
    "\n",
    "# --- 3) Parse Updated -> datetime robustly, then to quarter ---\n",
    "upd_clean = (df[updated_col].astype(str)\n",
    "             .str.replace(\"\\u00a0\", \" \", regex=False)\n",
    "             .str.replace(\"\\r\", \" \", regex=False)\n",
    "             .str.replace(\"\\n\", \" \", regex=False)\n",
    "             .str.strip())\n",
    "upd_dt = pd.to_datetime(upd_clean, errors='coerce')\n",
    "mask_na = upd_dt.isna()\n",
    "if mask_na.any():\n",
    "    fixed_2yy = upd_clean[mask_na].str.replace(\n",
    "        r'^(\\d{1,2})/(\\d{1,2})/(\\d{2})$', r'\\1/\\2/20\\3', regex=True\n",
    "    )\n",
    "    upd_dt.loc[mask_na] = pd.to_datetime(fixed_2yy, errors='coerce')\n",
    "\n",
    "df['Updated_dt'] = upd_dt\n",
    "df = df.dropna(subset=['Updated_dt'])\n",
    "df['quarter'] = df['Updated_dt'].dt.to_period('Q')\n",
    "\n",
    "# --- 4) Keep rows where Status & health equals COMPLETE (allow \"Completed\" too) ---\n",
    "is_complete = df[status_col].astype(str).str.fullmatch(r'complete(d)?', case=False, na=False)\n",
    "dfc = df.loc[is_complete, ['quarter', 'Product group']].copy()\n",
    "\n",
    "if dfc.empty:\n",
    "    print('No COMPLETE rows found.')\n",
    "else:\n",
    "    # --- 5) Count COMPLETE rows per quarter Ã— product group ---\n",
    "    counts = (dfc.groupby(['quarter', 'Product group'])\n",
    "                   .size()\n",
    "                   .unstack('Product group', fill_value=0))\n",
    "\n",
    "    # Ensure continuous quarter range (even if some quarters have zero)\n",
    "    all_quarters = pd.period_range(counts.index.min(), counts.index.max(), freq='Q')\n",
    "    counts = counts.reindex(all_quarters, fill_value=0)\n",
    "\n",
    "    # Order columns in a fixed logical order (use what exists)\n",
    "    desired_order = ['Event', 'Viewables', 'Home', 'Others', 'Wearable', 'Hearable']\n",
    "    present = [c for c in desired_order if c in counts.columns]\n",
    "    counts = counts[present]\n",
    "\n",
    "    # --- 6) Stacked bar plot (counts per group) ---\n",
    "    quarters = list(counts.index.astype(str))\n",
    "    x = np.arange(len(quarters))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Optional: stable unique colors for the six groups\n",
    "    palette = {\n",
    "        'Event':    '#1f77b4',\n",
    "        'Viewables':'#ff7f0e',\n",
    "        'Home':     '#2ca02c',\n",
    "        'Others':   '#9467bd',\n",
    "        'Wearable': '#8c564b',\n",
    "        'Hearable': '#e377c2',\n",
    "    }\n",
    "    bottom = np.zeros(len(counts), dtype=float)\n",
    "    handles = []\n",
    "    labels  = []\n",
    "    for col in counts.columns:\n",
    "        vals = counts[col].values\n",
    "        bars = ax.bar(x, vals, bottom=bottom, label=col, color=palette.get(col, None))\n",
    "        bottom += vals\n",
    "        handles.append(bars[0])\n",
    "        labels.append(col)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(quarters, rotation=45, ha='right')\n",
    "    ax.set_ylabel('Number of Complete Cases')\n",
    "    ax.set_title('Quarterly Number of Complete Cases by Main Product Lines')\n",
    "    ax.grid(True, axis='y', alpha=0.3)\n",
    "    ax.margins(x=0.02)\n",
    "\n",
    "    # Legend outside\n",
    "    plt.subplots_adjust(right=0.82, bottom=0.2)\n",
    "    ax.legend(handles=handles, labels=labels, title='Product Line',\n",
    "              loc='center left', bbox_to_anchor=(1.02, 0.5), frameon=True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspended.to_csv(\"suspended.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m pip install --upgrade openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python azure_gpt_csv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = tracker[tracker[\"Status & health\"] != \"DUPLICATE\"]\n",
    "mapping = {\n",
    "    \"COMPLETE\": \"COMPLETE\",\n",
    "    \"SUSPENDED\": \"INCOMPLETE\",\n",
    "    \"LOST\": \"INCOMPLETE\"\n",
    "}\n",
    "\n",
    "ml[\"classes\"] = ml[\"Status & health\"].map(mapping)\n",
    "\n",
    "# --- 1) Split data into features and target ---\n",
    "X = ml[[\"Time \\n(days)\"]]   # replace with the actual column name\n",
    "y = ml[\"classes\"]\n",
    "\n",
    "# --- 2) Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0, stratify=y\n",
    ")\n",
    "\n",
    "# --- 3) Define and fit Logistic Regression model ---\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- 4) Predictions ---\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "pos_class = \"COMPLETE\"\n",
    "y_prob = model.predict_proba(X_test)[:, list(model.classes_).index(pos_class)]\n",
    "\n",
    "# Compute ROC\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob, pos_label=pos_class)\n",
    "auc_score = roc_auc_score((y_test == pos_class).astype(int), y_prob)\n",
    "\n",
    "# Plot\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(f\"ROC-AUC Curve Using Time to Predict Status\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
